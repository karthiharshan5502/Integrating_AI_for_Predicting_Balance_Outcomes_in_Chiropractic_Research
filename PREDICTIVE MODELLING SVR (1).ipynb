{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9333695-fbea-4fe8-98f6-c58781cd07b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "df =  pd.read_csv('SUB_WEEKLY_FINAL_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4959164c-efb8-4373-a90a-8a55c47d65a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "df1 =  pd.read_csv('BalanceOutcomes.csv')\n",
    "df1 = df1.rename(columns={'Part': 'ID'})\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec1ce6f-a61f-400d-9725-d07239d1f174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "# Clean Task and Qual to avoid hidden issues\n",
    "df1['Task'] = df1['Task'].str.strip()\n",
    "df1['Qual'] = df1['Qual'].str.strip()\n",
    "df1['ID'] = df1['ID'].astype(int)\n",
    "\n",
    "# Define the weeks to extract\n",
    "weeks_to_extract = [0, 6, 12, 16]\n",
    "\n",
    "# Container for pivoted data\n",
    "pivoted_psr = []\n",
    "\n",
    "for week in weeks_to_extract:\n",
    "    temp = df1[df1['Week'] == week].copy()\n",
    "    \n",
    "    # Pivot: one row per ID, one column per Task+Qual\n",
    "    pivot = temp.pivot_table(index='ID', columns=['Task', 'Qual'], values='PS R')\n",
    "    \n",
    "    # Rename columns to indicate the week\n",
    "    pivot.columns = [f'PSR_W{week}_{task}_{qual}' for task, qual in pivot.columns]\n",
    "    \n",
    "    # Add to list\n",
    "    pivoted_psr.append(pivot)\n",
    "\n",
    "# Merge all weeks using OUTER join to retain all IDs\n",
    "psr_combined = reduce(\n",
    "    lambda left, right: pd.merge(left, right, left_index=True, right_index=True, how='outer'),\n",
    "    pivoted_psr\n",
    ")\n",
    "\n",
    "# Optional: reset index if you want 'ID' as a column\n",
    "psr_combined.reset_index(inplace=True)\n",
    "\n",
    "# Optional: fill missing values (if you want to treat missing scores as 0)\n",
    "# psr_combined.fillna(0, inplace=True)\n",
    "\n",
    "# ‚úÖ Done\n",
    "psr_combined.head()\n",
    "print(list(psr_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec133489-3c8e-412b-9d37-8ccbadef2c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 =  pd.read_csv('GaitOutcomes.csv')\n",
    "df3 = df3.rename(columns={'Part': 'ID'})\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "df3.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e90f3a7-f807-4329-b7ae-847dc6dd0baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "# Clean Task, Qual, and ID just once\n",
    "df3['Task'] = df3['Task'].str.strip()\n",
    "df3['Qual'] = df3['Qual'].str.strip()\n",
    "df3['ID'] = df3['ID'].astype(int)\n",
    "\n",
    "# Define weeks and gait-related variables\n",
    "weeks_to_extract = [0, 6, 12, 16]\n",
    "\n",
    "gait_variables = [\n",
    "    'Gait symmetry', 'Step length', 'Step length left', 'Step length right',\n",
    "    'Step time', 'Step time left', 'Step time right',\n",
    "    'Step length var', 'Step time var',\n",
    "    'Step length asym', 'Step time asym',\n",
    "    'Step velocity',\n",
    "    'Step count lap 1', 'Step count lap 2', 'Step count lap 3', 'Step count lap 4'\n",
    "]\n",
    "\n",
    "# Master container for all pivoted variables\n",
    "all_pivoted = []\n",
    "\n",
    "for var in gait_variables:\n",
    "    pivoted_list = []\n",
    "    for week in weeks_to_extract:\n",
    "        temp = df3[df3['Week'] == week].copy()\n",
    "        pivot = temp.pivot_table(index='ID', columns=['Task', 'Qual'], values=var)\n",
    "        \n",
    "        # Rename columns to reflect variable and week\n",
    "        pivot.columns = [f'{var.replace(\" \", \"\").replace(\"-\", \"\").replace(\"/\", \"\")}_W{week}_{task}_{qual}' for task, qual in pivot.columns]\n",
    "        pivoted_list.append(pivot)\n",
    "    \n",
    "    # Merge all weeks for this variable\n",
    "    merged_var = reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True, how='outer'), pivoted_list)\n",
    "    \n",
    "    all_pivoted.append(merged_var)\n",
    "\n",
    "# Merge all variables together on ID\n",
    "final_combined_gait = reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True, how='outer'), all_pivoted)\n",
    "\n",
    "# Optional: reset index\n",
    "final_combined_gait.reset_index(inplace=True)\n",
    "\n",
    "# ‚úÖ Preview the result\n",
    "final_combined_gait.head()\n",
    "#print(list(final_combined_gait))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641f00c9-1c2a-4e7a-930a-19261908f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Merge df with psr_combined\n",
    "merged1 = df[['ID', 'FIRST', 'MID', 'LAST', 'SLOPE1', 'SLOPE2', 'AGE', 'GROUP']].copy()\n",
    "merged1 = merged1.merge(psr_combined, on='ID', how='left')\n",
    "\n",
    "# Step 2: Merge with final_combined_gait\n",
    "final_merged = merged1.merge(final_combined_gait, on='ID', how='left')\n",
    "\n",
    "# Optional: check shape and preview\n",
    "print(\"‚úÖ Final merged shape:\", final_merged.shape)\n",
    "final_merged.head()\n",
    "print(list(final_merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811f970a-5d79-4343-a783-52025cb0f7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all columns\n",
    "all_cols = final_merged.columns\n",
    "\n",
    "# Step 1: Gait symmetry features (excluding the target)\n",
    "gait_cols = [col for col in all_cols \n",
    "             if col.startswith('Gaitsymmetry_') \n",
    "             and any(w in col for w in ['_W0_', '_W6_', '_W12_'])\n",
    "             and col != 'Gaitsymmetry_W12_Walk_HT']  # Exclude dependent var\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Step velocity features\n",
    "velocity_cols = [col for col in all_cols \n",
    "                 if col.startswith('Stepvelocity_') \n",
    "                 and any(w in col for w in ['_W0_', '_W6_', '_W12_'])]\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Other predictors\n",
    "other_predictors = ['FIRST', 'SLOPE1', 'MID', 'SLOPE2', 'LAST', 'AGE', 'GROUP']\n",
    "\n",
    "# Combine all predictors\n",
    "predictors = gait_cols + velocity_cols + other_predictors\n",
    "\n",
    "# Step 4: Final dataset\n",
    "df_gait = final_merged[predictors + ['Gaitsymmetry_W12_Walk_HT']].dropna()\n",
    "\n",
    "# One-hot encode categorical\n",
    "df_encoded = pd.get_dummies(df_gait, columns=['AGE', 'GROUP'], drop_first=True)\n",
    "\n",
    "# X and y\n",
    "X = df_encoded.drop(columns=['Gaitsymmetry_W12_Walk_HT']) #REMOVE DEPENDANT VARIABLE \n",
    "y = df_encoded['Gaitsymmetry_W12_Walk_HT'] # CHANGE DEPENDANT VARIABLE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c24e77f-68b5-4b96-a335-00154dea98f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 3: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 4: Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 5],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=5, scoring='r2', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "print(\"üîç Best Parameters:\", grid_search.best_params_)\n",
    "print(f\"‚úÖ R¬≤: {r2_score(y_test, y_pred):.4f}\")\n",
    "print(f\"‚úÖ RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}\")\n",
    "\n",
    "# Step 6: Cross-validation\n",
    "cv_scores = cross_val_score(best_rf, X_train, y_train, cv=5, scoring='r2')\n",
    "print(\"üìä Cross-Validated R¬≤ Scores:\", np.round(cv_scores, 4))\n",
    "print(f\"‚úÖ Average CV R¬≤: {np.mean(cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf30b91c-230f-443f-8f72-79f08dfa2530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and plot feature importance\n",
    "importances = best_rf.feature_importances_\n",
    "feat_names = X.columns\n",
    "\n",
    "feat_imp_df = pd.DataFrame({\n",
    "    'Feature': feat_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot top 20\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feat_imp_df['Feature'][:20], feat_imp_df['Importance'][:20])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Top 20 Feature Importances - Random Forest')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfd341f-0780-4333-a4d8-a9912a191b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# üìà Residuals vs Predicted\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(y_pred, residuals, alpha=0.7)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals vs Predicted\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# üìä Histogram of Residuals\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(residuals, bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.title(\"Histogram of Residuals\")\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# üìâ QQ Plot\n",
    "plt.figure(figsize=(6, 6))\n",
    "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.title(\"QQ Plot\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0afeb0c-9ecd-4edb-8bec-23424a2b75c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
